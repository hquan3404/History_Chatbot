{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b074ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Cleaned text saved to: E:\\HistoryChatbot\\data\\processed\\LSVN1954-1965_cleaned.txt\n",
      "Log saved to: E:\\HistoryChatbot\\data\\processed\\LSVN1954-1965_cleaning_log.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CẤU HÌNH ----------\n",
    "INPUT_FILE = r\"E:\\HistoryChatbot\\data\\raw\\text\\LSVN1954-1965.txt\"\n",
    "\n",
    "input_path = Path(INPUT_FILE)\n",
    "processed_dir = input_path.parents[2] / \"processed\"\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE = processed_dir / f\"{input_path.stem}_cleaned.txt\"\n",
    "LOG_FILE = processed_dir / f\"{input_path.stem}_cleaning_log.csv\"\n",
    "\n",
    "PAGE_STOP = \"15\"  # xóa đến khi gặp dòng chỉ chứa số này\n",
    "\n",
    "# Pattern tách page number lines\n",
    "PAGE_NUM_LINE = re.compile(r'^\\s*\\d{1,4}\\s*$')\n",
    "\n",
    "# Pattern cho block chú thích (trích dẫn sách)\n",
    "BIB_START = re.compile(r'^\\s*\\d+\\.\\s+.*?\\b(Nxb\\.?|, *tr\\.\\s*\\d[\\d\\-\\s]*)', re.IGNORECASE)\n",
    "# BIB_START = re.compile(r'(Nxb\\.?|, *tr\\.\\s*\\d[\\d\\-\\s]*)', re.IGNORECASE)\n",
    "# Header normalized regex\n",
    "HEADER_NORMALIZED_RE = re.compile(\n",
    "    r'^\\s*LICH\\s*SU\\s*VIET\\s*NAM\\s*[-–—]?\\s*TAP\\s*[0-9IVXLCDMIl\\|]{1,4}\\s*$',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "TITLE_RE = re.compile(r'(?i)\\bCHƯƠNG\\b|\\bPHẦN\\b|\\bMỤC\\b|\\bCHƯƠNG\\s+[IVXLCDM0-9]+\\b')\n",
    "\n",
    "MAX_HEADER_LEN = 120\n",
    "MIN_NONSPACE_CHARS = 5\n",
    "\n",
    "\n",
    "# ---------- HÀM TIỆN ÍCH ----------\n",
    "def strip_accents(s: str) -> str:\n",
    "    nfkd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "\n",
    "\n",
    "def is_lichsu_header(line: str) -> bool:\n",
    "    if not line or not line.strip():\n",
    "        return False\n",
    "    if len(line.strip()) > MAX_HEADER_LEN:\n",
    "        return False\n",
    "    norm = strip_accents(line).upper()\n",
    "    norm = re.sub(r'\\s+', ' ', norm).strip()\n",
    "    return bool(HEADER_NORMALIZED_RE.match(norm))\n",
    "\n",
    "\n",
    "def is_title_line(line: str) -> bool:\n",
    "    if not line or not line.strip():\n",
    "        return False\n",
    "    norm = strip_accents(line).upper()\n",
    "    return bool(TITLE_RE.search(norm))\n",
    "\n",
    "\n",
    "def is_references_start(line: str) -> bool:\n",
    "    if not line or not line.strip():\n",
    "        return False\n",
    "    norm = strip_accents(line).upper()\n",
    "    norm = re.sub(r'\\s+', ' ', norm).strip()\n",
    "    return bool(re.search(r'\\bTAI\\s*LIEU\\b.*\\bTHAM\\s*KHAO\\b', norm))\n",
    "\n",
    "\n",
    "def is_noise_line(line: str) -> bool:\n",
    "    if not line or not line.strip():\n",
    "        return False\n",
    "    if is_lichsu_header(line) or is_title_line(line):\n",
    "        return False\n",
    "\n",
    "    s = line.strip()\n",
    "    if re.match(r'^[\\-\\=\\_\\*\\.\\·\\•\\s]{1,}$', s):\n",
    "        return True\n",
    "\n",
    "    alnum_count = sum(1 for c in s if c.isalnum())\n",
    "    if alnum_count < MIN_NONSPACE_CHARS:\n",
    "        return True\n",
    "\n",
    "    prop_alnum = alnum_count / max(1, len(s))\n",
    "    if prop_alnum < 0.30:\n",
    "        return True\n",
    "\n",
    "    if len(s) < 30 and re.search(r'\\s{3,}', line):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# KIỂM TRA TIÊU ĐỀ CHƯƠNG\n",
    "def has_upper_block(lines: list[str], start: int, max_lines: int = 4) -> bool:\n",
    "    for j in range(1, max_lines + 1):\n",
    "        if start + j >= len(lines):\n",
    "            break\n",
    "        text = lines[start + j].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        letters = [c for c in text if c.isalpha()]\n",
    "        if letters:\n",
    "            ratio_upper = sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "            if ratio_upper >= 0.7:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def should_keep_chapter(lines: list[str], i: int) -> bool:\n",
    "    line = lines[i]\n",
    "    if re.match(r'^\\s*Chương\\b', line, re.IGNORECASE):\n",
    "        return has_upper_block(lines, i)\n",
    "    return True  # không phải dòng \"Chương\"\n",
    "\n",
    "\n",
    "# HÀNH ĐỘNG CHÍNH\n",
    "def clean_book(input_path: str, output_path: Path, log_path: Path):\n",
    "    raw = Path(input_path).read_text(encoding='utf-8')\n",
    "    original_lines = raw.splitlines()\n",
    "    removed_records = []\n",
    "\n",
    "    cut_index = None\n",
    "    for i, ln in enumerate(original_lines):\n",
    "        if PAGE_NUM_LINE.match(ln) and ln.strip() == PAGE_STOP:\n",
    "            cut_index = i\n",
    "            break\n",
    "\n",
    "    if cut_index is not None:\n",
    "        removed_records.append({\n",
    "            \"line_no\": 0,\n",
    "            \"original\": f\"removed all content before page {PAGE_STOP}\",\n",
    "            \"reason\": \"REMOVE_FIRST_PAGES_UNTIL_15\"\n",
    "        })\n",
    "        offset = cut_index + 1\n",
    "        lines = original_lines[offset:]\n",
    "    else:\n",
    "        removed_records.append({\n",
    "            \"line_no\": 0,\n",
    "            \"original\": f\"page marker {PAGE_STOP} not found; no front-pages removed\",\n",
    "            \"reason\": \"NO_PAGE_MARKER\"\n",
    "        })\n",
    "        offset = 0\n",
    "        lines = original_lines\n",
    "\n",
    "    cleaned = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        ln = lines[i]\n",
    "        orig_line_no = offset + i\n",
    "\n",
    "        if is_references_start(ln):\n",
    "            for k in range(i, len(lines)):\n",
    "                removed_records.append({\n",
    "                    \"line_no\": offset + k,\n",
    "                    \"original\": lines[k],\n",
    "                    \"reason\": \"REFERENCES_SECTION_REMOVED\"\n",
    "                })\n",
    "            break\n",
    "\n",
    "        if is_lichsu_header(ln):\n",
    "            removed_records.append({\n",
    "                \"line_no\": orig_line_no,\n",
    "                \"original\": ln,\n",
    "                \"reason\": \"LICH_SU_HEADER_REMOVED\"\n",
    "            })\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # loại bỏ trích dẫn sách\n",
    "        if BIB_START.match(ln):\n",
    "            block = [ln]\n",
    "            j = i + 1\n",
    "            while j < len(lines) and lines[j].strip() != \"\":\n",
    "                block.append(lines[j])\n",
    "                j += 1\n",
    "            for k, b in enumerate(block):\n",
    "                removed_records.append({\n",
    "                    \"line_no\": offset + i + k,\n",
    "                    \"original\": b,\n",
    "                    \"reason\": \"BIBLIOGRAPHIC_BLOCK_REMOVED\"\n",
    "                })\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        # xử lý \"Chương\" → giữ hoặc xóa\n",
    "        if re.match(r'^\\s*Chương\\b', ln, re.IGNORECASE):\n",
    "            if not should_keep_chapter(lines, i):\n",
    "                removed_records.append({\n",
    "                    \"line_no\": orig_line_no,\n",
    "                    \"original\": ln,\n",
    "                    \"reason\": \"DUPLICATE_CHAPTER_HEADER_REMOVED\"\n",
    "                })\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        cleaned.append(ln)\n",
    "        i += 1\n",
    "\n",
    "    intermediate = []\n",
    "    for idx, ln in enumerate(cleaned):\n",
    "        if PAGE_NUM_LINE.match(ln):\n",
    "            removed_records.append({\n",
    "                \"line_no\": offset + idx,\n",
    "                \"original\": ln,\n",
    "                \"reason\": \"ISOLATED_PAGE_NUMBER_REMOVED\"\n",
    "            })\n",
    "            continue\n",
    "        intermediate.append(ln)\n",
    "\n",
    "    final_lines = []\n",
    "    prev_blank = False\n",
    "    for idx, ln in enumerate(intermediate):\n",
    "        original_ln_no = offset + idx\n",
    "        if not ln.strip():\n",
    "            if not prev_blank:\n",
    "                final_lines.append('')\n",
    "                prev_blank = True\n",
    "            else:\n",
    "                removed_records.append({\n",
    "                    \"line_no\": original_ln_no,\n",
    "                    \"original\": ln,\n",
    "                    \"reason\": \"EXCESSIVE_BLANK_REMOVED\"\n",
    "                })\n",
    "            continue\n",
    "        else:\n",
    "            prev_blank = False\n",
    "\n",
    "        if is_noise_line(ln):\n",
    "            removed_records.append({\n",
    "                \"line_no\": original_ln_no,\n",
    "                \"original\": ln,\n",
    "                \"reason\": \"NOISE_LINE_REMOVED\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        final_lines.append(ln)\n",
    "\n",
    "    output_path.write_text(\"\\n\".join(final_lines), encoding='utf-8')\n",
    "\n",
    "    with open(log_path, \"w\", encoding='utf-8', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"line_no\", \"original\", \"reason\"], quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "        for rec in removed_records:\n",
    "            writer.writerow(rec)\n",
    "\n",
    "    print(\"Done. Cleaned text saved to:\", output_path)\n",
    "    print(\"Log saved to:\", log_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_book(INPUT_FILE, OUTPUT_FILE, LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xử lý xong.\n",
      " - Văn bản sạch: output.txt\n",
      " - Các dòng đã xóa: removed.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Iterable, Set, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "TR_PATTERN = re.compile(r'tr\\.', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "\n",
    "def _mark_window(index: int, total: int, neighbor: int) -> Iterable[int]:\n",
    "    start = max(0, index - neighbor)\n",
    "    end = min(total - 1, index + neighbor)\n",
    "    return range(start, end + 1)\n",
    "\n",
    "\n",
    "def clean_bibliographic_lines(text: str, neighbor: int = 2) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Xóa mọi dòng chứa 'tr.' (không phân biệt hoa/thường),\n",
    "    đồng thời xóa luôn `neighbor` dòng trên và dưới (mặc định 2).\n",
    "    Trả về tuple: (cleaned_text, removed_text).\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text, \"\"\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    total = len(lines)\n",
    "    to_delete: Set[int] = set()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if TR_PATTERN.search(line):\n",
    "            for idx in _mark_window(i, total, neighbor):\n",
    "                to_delete.add(idx)\n",
    "\n",
    "    result_lines = [ln for i, ln in enumerate(lines) if i not in to_delete]\n",
    "    removed_lines = [ln for i, ln in enumerate(lines) if i in to_delete]\n",
    "\n",
    "    return \"\\n\".join(result_lines), \"\\n\".join(removed_lines)\n",
    "\n",
    "\n",
    "def clean_file(input_path: str, output_path: str, removed_path: str, neighbor: int = 2) -> None:\n",
    "    \"\"\"\n",
    "    Đọc văn bản từ file input_path, làm sạch,\n",
    "    và ghi kết quả ra file output_path,\n",
    "    đồng thời lưu các dòng bị xóa ra removed_path.\n",
    "    \"\"\"\n",
    "    in_file = Path(input_path)\n",
    "    if not in_file.exists():\n",
    "        raise FileNotFoundError(f\"Không tìm thấy file: {input_path}\")\n",
    "\n",
    "    text = in_file.read_text(encoding=\"utf-8\")\n",
    "    cleaned, removed = clean_bibliographic_lines(text, neighbor=neighbor)\n",
    "\n",
    "    Path(output_path).write_text(cleaned, encoding=\"utf-8\")\n",
    "    Path(removed_path).write_text(removed, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ví dụ: đọc input.txt, ghi kết quả vào output.txt, dòng đã xóa vào removed.txt\n",
    "    clean_file(r\"E:\\HistoryChatbot\\data\\processed\\LSVN1954-1965_cleaned.txt\", \"output.txt\", \"removed.txt\", neighbor=1)\n",
    "    print(\"Đã xử lý xong.\")\n",
    "    print(\" - Văn bản sạch: output.txt\")\n",
    "    print(\" - Các dòng đã xóa: removed.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEEP\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_upper_block(lines: list[str], start: int, max_lines: int = 4) -> bool:\n",
    "    \"\"\"\n",
    "    Kiểm tra sau 'Chương' có block full chữ hoa không.\n",
    "    \"\"\"\n",
    "    for j in range(1, max_lines + 1):\n",
    "        if start + j >= len(lines):\n",
    "            break\n",
    "        text = lines[start + j].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        letters = [c for c in text if c.isalpha()]\n",
    "        if letters:\n",
    "            ratio_upper = sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "            if ratio_upper >= 0.7:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def check_chapter_block(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Trả về:\n",
    "      - 'KEEP' nếu là tiêu đề chương gốc (sau 'Chương' có block chữ hoa),\n",
    "      - 'REMOVE' nếu chỉ là header lặp,\n",
    "      - 'NO_CHAPTER_FOUND' nếu không có 'Chương'.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^\\s*Chương\\b', line, re.IGNORECASE):\n",
    "            return \"KEEP\" if is_upper_block(lines, i) else \"REMOVE\"\n",
    "    return \"NO_CHAPTER_FOUND\"\n",
    "\n",
    "\n",
    "# ----------------- DEMO -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"\n",
    "Chương I \n",
    "\n",
    "“MIỄN BÁC TRONG THỜI KỲ \n",
    "KHÔI PHỤC, CẢI TẠO VÀ BƯỚC ĐÀU \n",
    "PHÁT TRIẾN KINH TÉ, VĂN HÓA \n",
    "(1954-1960) \n",
    "    \"\"\"\n",
    "    result = check_chapter_block(text)\n",
    "    print(result)  # -> \"KEEP\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba26686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xử lý xong. Kết quả lưu tại: output.txt\n"
     ]
    }
   ],
   "source": [
    "def normalize_newlines(input_file: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Chuẩn hóa file văn bản: bỏ xuống dòng lộn xộn,\n",
    "    chỉ giữ khoảng trắng khi là ngắt đoạn.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Thay nhiều khoảng trắng dòng -> 2 xuống dòng\n",
    "    import re\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)  # gộp khoảng trắng thừa\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)  # >2 xuống dòng -> 2\n",
    "    text = re.sub(r\"([^\\n])\\n([^\\n])\", r\"\\1 \\2\", text)  # nối dòng ngắt sai\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = r\"E:\\HistoryChatbot\\data\\processed\\LSVN1954-1965_cleaned_ver2.txt\"\n",
    "    output_path = \"output.txt\"\n",
    "    normalize_newlines(input_path, output_path)\n",
    "    print(f\"Đã xử lý xong. Kết quả lưu tại: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fe21c",
   "metadata": {},
   "source": [
    "### Một số vấn đề khi lưu lại dữ liệu vào json:\n",
    "- đang bị nhầm lẫn các tiêu đề ở phần subsection, section, chapter, hiện tại đang bị nhầm lẫn ở 2 chỗ:\n",
    "    - Nhầm lẫn với phần trích dẫn sách còn sót lại sau khi xử lý dữ liệu => khả năng vẫn xóa hết phần trích dẫn nào có tr...\n",
    "    - Nhầm lẫn với một vài đoạn văn bản mà đầu văn bản được đưa bởi số liệu\n",
    "- Chapter đang bị lặp bởi trong sách nó nằm ở phần tiêu đề phía trên ở mỗi trang(có thể xóa được, tuy nhiên cần phải biết được xóa ở đâu để tránh nhầm lẫn với tiêu đề Chương gốc), nếu sau Chương.. là một đoạn text full chữ hoa thì giữ lại, là tiêu đề chương, còn lại là nhiễu\n",
    "(5068)(5217)\n",
    "- trước khi xóa có lẽ cần co dòng lại cho hợp format\n",
    "- REFERENCES_SECTION_REMOVED đang bị xóa nhầm cả với các tiêu đề thay vì chỉ xóa mình chú thích"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
